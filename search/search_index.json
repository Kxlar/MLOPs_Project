{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Anomaly Detection (MVTec AD)","text":""},{"location":"#overview","title":"Overview","text":"<p>This project implements a zero-shot anomaly detection pipeline for industrial visual inspection using the MVTec AD dataset, with a focus on the Carpet class.</p> <p>The goal is to detect anomalous regions in images without using any defective samples during training, making the approach suitable for real-world industrial settings where defects are rare or unknown in advance.</p>"},{"location":"#why-zero-shot-anomaly-detection","title":"Why Zero-Shot Anomaly Detection?","text":"<p>In manufacturing, collecting and labeling defective samples is often expensive, time-consuming, or impractical. Zero-shot anomaly detection addresses this challenge by learning only from normal (defect-free) data and identifying deviations at inference time.</p> <p>This project explores whether vision foundation models can enable robust anomaly detection without pre-training nor task-specific fine-tuning.</p> <p></p>"},{"location":"#method","title":"Method","text":"<p>The pipeline is built around DINOv3, a self-supervised Vision Transformer:</p> <ol> <li>Extract patch-level features from normal training images</li> <li>Build a memory bank of normal visual patterns</li> <li>Compare test image features against the memory bank using distance-based scoring</li> <li>Produce:    -- Image-level anomaly scores    -- Pixel-level anomaly heatmaps</li> </ol>"},{"location":"#features","title":"Features","text":"<p>The project supports:</p> <ul> <li>Zero-shot feature extraction using DINOv3</li> <li>Memory bank construction from normal samples</li> <li>Pixel-level anomaly heatmaps</li> <li>Image-level &amp; pixel-level evaluation (ROC AUC)</li> <li>Data augmentation and drift simulation</li> <li>Dockerized training and inference</li> <li>FastAPI backend and frontend</li> <li>Cloud deployment (GCP-ready)</li> <li>Reproducible experiments with Hydra</li> <li>among other MLOps implementations</li> </ul>"},{"location":"#dataset","title":"Dataset","text":"<p>We use the MVTec Anomaly Detection (MVTec AD) dataset, a standard benchmark for industrial anomaly detection.</p> <ul> <li>Training set: normal images only</li> <li>Test set: normal + defective images with pixel-level ground truth masks</li> <li>Focus class: Carpet</li> </ul>"},{"location":"#project-context","title":"Project Context","text":"<p>This project was developed as part of the 02476 MLOps course at DTU.</p>"},{"location":"#team-members","title":"Team Members","text":"<ul> <li>Lea</li> <li>Pierre-Eduard</li> <li>Loic (aka Cloud Wizard)</li> <li>Jawhara</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide helps you run the anomaly detection project locally as quickly as possible. It assumes no prior knowledge of the codebase.</p>"},{"location":"getting-started/#1-system-requirements","title":"1. System Requirements","text":"<ul> <li>Linux / Windows /   (We recommend that Windows users use WSL)</li> <li>Python \u2265 3.11 (3.13 recommended)</li> <li>Docker</li> <li>Git</li> </ul>"},{"location":"getting-started/#2-installation","title":"2. Installation","text":"<p>Clone the repository: <pre><code>git clone https://github.com/Kxlar/MLOPs_Project.git\n</code></pre></p>"},{"location":"getting-started/#3-data-and-pre-trained-model","title":"3. Data and Pre-trained model","text":"<p>Downloads the data and the pretrained model from GCP bucket</p> <p><pre><code>chmod +x setup.sh\n</code></pre> <pre><code>./setup.sh\n</code></pre></p> <p>Install dependencies: <pre><code>uv sync\n</code></pre> Verify installation: <pre><code>uv run python -c \"import torch; print('OK')\"\n</code></pre></p>"},{"location":"getting-started/#4-project-structure","title":"4. Project Structure","text":"<pre><code>MLOPs_Project/\n\u251c\u2500\u2500 src/anomaly_detection/   # Core pipeline (training, inference, API)\n\u251c\u2500\u2500 configs/                 # Hydra experiment configurations\n\u251c\u2500\u2500 dockerfiles/             # Dockerfiles per pipeline stage\n\u251c\u2500\u2500 docs/                    # MkDocs documentation\n\u251c\u2500\u2500 data/                    # Input datasets (not versioned)\n\u251c\u2500\u2500 models/                  # Saved memory banks / model artifacts\n\u251c\u2500\u2500 logs/                    # Runtime logs\n\u251c\u2500\u2500 results/                 # Evaluation outputs\n\u2514\u2500\u2500 reports/                 # Generated reports\n</code></pre>"},{"location":"getting-started/#invoke-tasks","title":"Invoke tasks","text":"<pre><code>uv run invoke --list\n</code></pre>"},{"location":"getting-started/#5-minimal-end-to-end-run","title":"5. Minimal End-to-End Run","text":""},{"location":"getting-started/#51-data-loading","title":"5.1 Data loading:","text":"<p>The project expects the MVTec AD dataset in the following structure:</p> data.py<pre><code>uv run python src/anomaly_detection/data.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --img_size 224 \\\n  --batch_size 8\n</code></pre>"},{"location":"getting-started/#52-build-the-memory-bank","title":"5.2 Build the memory bank:","text":"train.py<pre><code>uv run python src/anomaly_detection/train.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth \\\n  --save_path ./models/memory_bank.pt \\\n  --img_size 224 \\\n  --batch_size 8\n</code></pre> <p>This step: - extracts features - builds a memory bank - saves model artifacts under models/</p>"},{"location":"getting-started/#53-run-inference","title":"5.3 Run inference:","text":"inference.py<pre><code>uv run python src/anomaly_detection/inference.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth \\\n  --memory_bank_path ./models/memory_bank.pt \\\n  --output_dir ./results/figures \\\n  --img_size 224 \\\n  --batch_size 8 \\\n  --k 1\n</code></pre> <p>Outputs include: - anomaly heatmaps - anomaly scores</p>"},{"location":"getting-started/#54-evaluate-the-model","title":"5.4 Evaluate the model:","text":"evaluate.py<pre><code>uv run python src/anomaly_detection/evaluate.py \\\n--data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth \\\n  --memory_bank_path ./models/memory_bank.pt \\\n  --output_dir ./results/figures/eval \\\n  --img_size 224 \\\n  --batch_size 8 \\\n  --k 10\n</code></pre> <p>Evaluation outputs are written to: - results/figures/eval</p>"},{"location":"getting-started/#6-run-the-api","title":"6. Run the API","text":"<p>To start the FastAPI backend locally: <pre><code>uv run python src/anomaly_detection/API/api.py\n</code></pre></p> <p>Open in browser: <pre><code>http://localhost:8000\n</code></pre></p> <p>Health check: <pre><code>curl http://localhost:8000/health\n</code></pre></p>"},{"location":"getting-started/#7-run-with-docker","title":"7. Run with Docker","text":"<p>Build and run the backend API using Docker: <pre><code>docker build -f dockerfiles/backend.dockerfile -t anomaly-backend .\ndocker run -p 8000:8000 anomaly-backend\n</code></pre></p>"},{"location":"getting-started/#8-run-bentoml-api","title":"8. Run BentoML API","text":"<p>Build and run the backend API using BentoML.</p> <p>Convert our model into a ONNX model: <pre><code>uv run export_onnx_paranoid.py\n</code></pre></p> <p>Initialize BentoML: <pre><code>uv run bentoml --version\n</code></pre></p> <p>Build the docker container: <pre><code>uv run bentoml build\nbentoml containerize anomaly_detection_service:latest\n</code></pre></p> <p>Copy the container tag and run the following command to launch the API: <pre><code>docker run --rm -p 3000:3000 anomaly_detection_service:TAG\n</code></pre></p>"},{"location":"deployment/cloud/","title":"Cloud (GCP)","text":"<p>The application was deployed on Google Cloud Run using a two-service architecture, separating the frontend and the backend components.</p> <p>The frontend service provides the user interface and is exposed through a public URL automatically generated by Cloud Run:</p> <p><pre><code>https://frontend-445436263618.europe-west1.run.app/\n</code></pre> which is used by users to interact with the application. The final version works with the BentoML backend service, as it provides a robust and scalable solution for serving the machine learning model. The frontend communicates with the backend through its Cloud Run endpoint, enabling a clean separation of concerns and independent monitoring of each service.</p>"},{"location":"deployment/docker/","title":"Docker Setup","text":"<p>This project uses multiple Dockerfiles, each designed for a specific stage of the MLOps workflow: training, inference, evaluation, backend API serving, frontend UI, and an end-to-end data-drift demo.</p> <p>All containers are reproducible and lockfile-driven, primarily using uv with uv.lock and pyproject.toml.</p> Dockerfile Purpose Typical Usage <code>backend.dockerfile</code> FastAPI backend (API server) Local &amp; cloud inference <code>frontend.dockerfile</code> Streamlit UI Local &amp; Cloud Run <code>train.dockerfile</code> Model training Local / CI / cloud jobs <code>evaluation.dockerfile</code> Model evaluation Reproducible metrics <code>inference.dockerfile</code> Batch inference Heatmaps &amp; overlays <code>data_drift_demo.dockerfile</code> End-to-end drift demo Experimental pipeline <p>Note</p> <p>Since these files are not named <code>Dockerfile</code>, you must always use <code>-f</code> when building.</p>"},{"location":"deployment/docker/#1-backend-api-fastapi-backenddockerfile","title":"1. Backend API (FastAPI) \u2014 <code>backend.dockerfile</code>","text":""},{"location":"deployment/docker/#build-run","title":"Build &amp; run","text":"<p><pre><code>docker build -f backend.dockerfile -t anomaly-backend .\ndocker run --rm -p 8000:8000 anomaly-backend\n</code></pre> Entrypoint</p> <pre><code>uv run uvicorn src.anomaly_detection.api:app --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"deployment/docker/#2-backend-api-bento","title":"2. Backend API (Bento)","text":"<p>We experimented with a BentoML backend but ultimately settled on FastAPI for simplicity and consistency with the rest of the stack.</p> <p>For BentoML app to do inference:</p> <ol> <li>Initialize bentoml <pre><code>uv run bentoml --version\n</code></pre></li> <li>Convert the torch model into a ONNX model <pre><code>uv run export_onnx_paranoid.py\n</code></pre></li> <li>Build the bento container <pre><code>uv run bentoml build\n</code></pre></li> <li>Launch docker in the background. bBuild the docker container based on the bento - replace TAG by the number that shows up after step 3 <pre><code>bentoml containerize anomaly_detection_service:&lt;TAG&gt;\n</code></pre></li> <li> <p>Run the container <pre><code>docker run --rm -p 3000:3000 anomaly_detection_service:&lt;TAG&gt;\n</code></pre></p> </li> <li> <p>To use the backend, go to http://localhost:3000 </p> </li> </ol> <p>OR (once the docker image is running) in new terminal run: <pre><code>uv run src/anomaly_detection/api_inference.py\n</code></pre></p>"},{"location":"deployment/docker/#3-frontend-ui-streamlit-frontenddockerfile","title":"3. Frontend UI (Streamlit)  \u2014 <code>frontend.dockerfile</code>","text":"<p>This project includes a lightweight Streamlit frontend that allows users to upload an image and run anomaly detection through the backend API.</p>"},{"location":"deployment/docker/#1-running-the-backend-required","title":"1. Running the Backend (Required)","text":"<p>The frontend expects the backend to be available at: <pre><code>http://127.0.0.1:8000\n</code></pre></p> <p>Start the backend in a dedicated terminal and keep it running:</p> <pre><code>cd ~/projects/MLOPs_Project\nuv run uvicorn src.anomaly_detection.api:app --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"deployment/docker/#2-running-the-frontend-streamlit","title":"2. Running the Frontend (Streamlit)","text":"<p>Open a second terminal and run:</p> <pre><code>cd ~/projects/MLOPs_Project\nuv run streamlit run frontend/frontend.py \\\n  --server.port 8501 \\\n  --server.address 0.0.0.0\n</code></pre> <p>You should see:</p> <p>You can now view your Streamlit app in your browser. URL: http://0.0.0.0:8501</p> <p>Open in your browser: <pre><code>http://localhost:8501\n</code></pre></p>"},{"location":"deployment/docker/#4-training-traindockerfile","title":"4. Training \u2014 <code>train.dockerfile</code>","text":"<p>Runs the training pipeline in a fully reproducible container.</p>"},{"location":"deployment/docker/#build-run_1","title":"Build &amp; run","text":"<pre><code>docker build -f train.dockerfile -t anomaly-train .\ndocker run --rm anomaly-train\n</code></pre>"},{"location":"deployment/docker/#5-evaluation-evaluationdockerfile","title":"5. Evaluation  \u2014 <code>evaluation.dockerfile</code>","text":"<p>Runs model evaluation on a dataset to compute metrics in a controlled and reproducible environment.</p>"},{"location":"deployment/docker/#build-run_2","title":"Build &amp; run","text":"<pre><code>docker build -f evaluation.dockerfile -t anomaly-eval .\ndocker run --rm anomaly-eval\n</code></pre>"},{"location":"deployment/docker/#6-inference-inferencedockerfile","title":"6. Inference \u2014 <code>inference.dockerfile</code>","text":"<p>Runs offline inference tasks such as heatmap generation, overlay visualization and batch prediction.</p>"},{"location":"deployment/docker/#build-run_3","title":"Build &amp; run","text":"<pre><code>docker build -f inference.dockerfile -t anomaly-infer .\ndocker run --rm anomaly-infer\n</code></pre>"},{"location":"deployment/docker/#7-data-drifting-demo-data_drift_demodockerfile","title":"7. Data drifting Demo \u2014 <code>data_drift_demo.dockerfile</code>","text":"<p>Runs an end-to-end data drift experiment at container runtime via an entrypoint script (not during image build).</p> <p>Details are provided in Pipeline \u2192 Data Drift.</p>"},{"location":"development/hydra/","title":"Hydra","text":""},{"location":"development/hydra/#run-hydra-scripts","title":"Run (Hydra scripts)","text":"<pre><code>uv sync\nuv run python -m anomaly_detection.hydra.train_hydra\nuv run python -m anomaly_detection.hydra.evaluate_hydra\nuv run python -m anomaly_detection.hydra.inference_hydra\nuv run python -m anomaly_detection.hydra.augment_hydra\n</code></pre>"},{"location":"development/hydra/#override-parameters-example","title":"Override parameters (example)","text":"<pre><code>uv run src/anomaly_detection/evaluate_hydra.py class_name=carpet k=5 output_dir=./results_k5\n</code></pre>"},{"location":"development/logging-profiling/","title":"Profiling","text":"<p>Profiler used: cProfile</p> <p>Target script:  evaluate.py</p> <p>The evaluation script was profiled using Python\u2019s built-in cProfile, sorted by cumulative runtime:</p> <pre><code>rm -f evaluate.prof\n\nuv run python -m cProfile -s cumulative -o evaluate.prof \\\n  src/anomaly_detection/evaluate.py \\\n  --data_root ./data/raw \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth \\\n  --memory_bank_path ./models/memory_bank.pt \\\n  --output_dir ./reports/figures/eval \\\n  --img_size 224 \\\n  --batch_size 8 \\\n  --k 10\n</code></pre> <p>To inspect the most time-consuming functions:</p> <pre><code>uv run python -c \"import pstats; p=pstats.Stats('evaluate.prof'); p.sort_stats('cumulative').print_stats(20)\"\n</code></pre> <p>Visualization The profiling results were visualized using SnakeViz:</p> <pre><code>uv add snakeviz\nuv run snakeviz evaluate.prof\n</code></pre>"},{"location":"development/style/","title":"Style &amp; Linting","text":""},{"location":"development/style/#tools","title":"Tools","text":"<ul> <li>Ruff: Fast Python linter and formatter.</li> <li>pre-commit: Runs checks automatically at commit/push.</li> <li>Pytest + Coverage: Executed by a pre-commit hook to keep tests green.</li> </ul> <p>Configuration lives in <code>pyproject.toml</code> and <code>.pre-commit-config.yaml</code>.</p>"},{"location":"development/style/#format","title":"Format","text":"<ul> <li>Check formatting (no changes): <pre><code>uv run ruff format --check .e\n</code></pre></li> <li>Auto-format (apply changes): <pre><code>uv run ruff format .\n</code></pre></li> </ul>"},{"location":"development/style/#lint","title":"Lint","text":"<ul> <li>Lint entire repo: <pre><code>uv run ruff check .\n</code></pre></li> <li>Apply autofixes for fixable lint rules: <pre><code>uv run ruff check . --fix\n</code></pre></li> </ul>"},{"location":"development/style/#pre-commit-hooks-on-commitpush","title":"pre-commit Hooks (on commit/push)","text":"<p>Hooks are defined in <code>.pre-commit-config.yaml</code>. They include: - General hygiene: trailing whitespace, end-of-file, YAML check, large files - Ruff lint with <code>--fix</code> and <code>ruff-format</code> - Unit tests via <code>pytest</code></p> <p>Install hooks so they run automatically on <code>git commit</code>: <pre><code>uv run pre-commit install\n</code></pre></p>"},{"location":"pipeline/data/","title":"Data","text":"<p>This project uses the MVTec AD dataset.</p>"},{"location":"pipeline/data/#1-downloading-dataset","title":"1. Downloading Dataset","text":"<p>Our dataset can be downloaded from the cloud</p> <p><pre><code>chmod +x setup.sh\n</code></pre> <pre><code>./setup.sh\n</code></pre></p>"},{"location":"pipeline/data/#2-data-module-documentation","title":"2. Data Module Documentation","text":"<p>The data.py script handles data loading, preprocessing, and synthetic data generation (drift simulation). It manages the MVTec AD dataset for both training (normal only) and testing (normal + anomalies).</p> <ul> <li>Dataset Handling: Manages train/test splits, ensuring only \"good\" data is used for training while the test set includes anomalies with binary labels.</li> <li>Preprocessing: Applies resizing and normalization using ImageNet statistics.</li> <li>Augmentation Pipeline: Supports on-the-fly or offline augmentations (Rotation, Color Jitter, Blur) to simulate data drift or increase robustness.</li> </ul>"},{"location":"pipeline/data/#what-it-does","title":"What it does:","text":"<ol> <li>Scans the file system to load MVTec images (train/test splits).</li> <li>Applies preprocessing (resizing, normalization) and optional augmentations (rotation, blur, color jitter).</li> <li>Loads pixel-level ground-truth masks for defect evaluation.</li> <li>(CLI mode) Generates and saves a synthetic \"drifted\" dataset to disk for robustness testing.</li> </ol>"},{"location":"pipeline/data/#arguments","title":"Arguments","text":"<p>--data_root: Path to the root directory of the dataset.</p> <p>--class_name: The object category to analyze (e.g., carpet).</p> <p>--img_size: Target image size for resizing (default: 224).</p> <p>--batch_size: Batch size for data loading (default: 8).</p> <p>--augment: Flag to enable data augmentation.</p> <p>--aug_types: List of augmentations to apply. Choices: rotation, color, blur.</p> <p>--aug_multiplier: Number of augmentations per image (only used in save mode).</p> <p>--rot_degrees: Maximum degrees for rotation augmentation.</p> <p>--blur_kernel: Kernel size for Gaussian blur.</p> <p>--save_aug_path: Path to save the augmented images. If provided, the script runs in \"save mode\" instead of loading mode.</p> <p>--save_aug_dataset_name: Name of the output dataset folder.</p> <p>--split: Which split to process/save (train or test).</p> <p>--include_anomalies: Flag to include defect folders when generating drifted data for the test split.</p>"},{"location":"pipeline/data/#verify-data-loading","title":"Verify Data Loading","text":"<p>Run this to initialize the dataloaders and check sample counts without saving any files. This confirms the dataset structure is correct.</p> <pre><code>uv run python src/anomaly_detection/data.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --img_size 224 \\\n  --batch_size 8\n</code></pre>"},{"location":"pipeline/data/#folder-structure","title":"Folder structure","text":"<p>Dataset folder structure:</p> <pre><code>data/\n\u2514\u2500\u2500 carpet/\n    \u251c\u2500\u2500 train/\n    \u2502   \u2514\u2500\u2500 good/\n    \u2502       \u251c\u2500\u2500 000.png\n    \u2502       \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 test/\n    \u2502   \u251c\u2500\u2500 good/\n    \u2502   \u251c\u2500\u2500 cut/\n    \u2502   \u251c\u2500\u2500 hole/\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 ground_truth/\n        \u251c\u2500\u2500 cut/\n        \u2502   \u251c\u2500\u2500 000_mask.png\n        \u2502   \u2514\u2500\u2500 ...\n        \u251c\u2500\u2500 hole/\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"pipeline/data/#generate-a-drifted-dataset-data-augmentation","title":"Generate a \"Drifted\" Dataset (data augmentation)","text":"<p>Run this to apply augmentations (color, blur and rotation) to the test set and save the images to disk. This is useful for testing model robustness against domain shifts.</p> <pre><code>uv run ./src/anomaly_detection/data.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --save_aug_path ./data/augmented \\\n  --save_aug_dataset_name carpet_augmented \\\n  --split test \\\n  --augment \\\n  --aug_types rotation color blur \\\n  --aug_multiplier 1 \\\n  --rot_degrees 20 \\\n  --color_brightness 0.2 \\\n  --color_contrast 0.2 \\\n  --blur_kernel 3 \\\n  --include_anomalies\n</code></pre>"},{"location":"pipeline/data_drift/","title":"Data Drift &amp; Data Augmentation Demo","text":"<p>This page describes our data drift experiment and how we evaluate the effect of rebuilding the memory bank after distribution shifts in the data.</p> <p>The goal is to demonstrate how data augmentation\u2013induced drift affects model outputs, and whether retraining on drifted data improves robustness.</p>"},{"location":"pipeline/data_drift/#motivation-why-data-drift","title":"Motivation: Why Data Drift?","text":"<p>In real-world deployments, data distributions often change over time due to:</p> <ul> <li>Lighting changes</li> <li>Color shifts</li> <li>Sensor degradation</li> <li>Environmental variation</li> <li>Domain transfer</li> </ul> <p>This phenomenon is known as data drift.</p> <p>Our anomaly detection model is trained on normal images only using a DINOv3-based memory bank. We want to study what happens when:</p> <ol> <li>The test data is drifted</li> <li>The training data is also drifted</li> <li>The memory bank is rebuilt on drifted data</li> </ol>"},{"location":"pipeline/data_drift/#experimental-idea","title":"Experimental Idea","text":"<p>We compare two scenarios:</p>"},{"location":"pipeline/data_drift/#before-no-adaptation","title":"Before (No Adaptation)","text":"<ul> <li>Test data is artificially drifted</li> <li>Inference uses a memory bank built on original training data</li> <li>Expectation: higher anomaly scores / distribution shift</li> </ul>"},{"location":"pipeline/data_drift/#after-with-adaptation","title":"After (With Adaptation)","text":"<ul> <li>Training data is drifted using the same transformation</li> <li>Memory bank is rebuilt on drifted training data</li> <li>Test inference is repeated</li> <li>Expectation: anomaly score distribution shifts back toward normal</li> </ul>"},{"location":"pipeline/data_drift/#high-level-pipeline","title":"High-Level Pipeline","text":"<pre><code>Generate drifted test data\n        \u2193\nInference on drifted test data\n        \u2193\nHistogram (\"Before\")\n        \u2193\nGenerate drifted training data\n        \u2193\nRebuild memory bank\n        \u2193\nInference on drifted test data again\n        \u2193\nHistogram (\"After\")\n        \u2193\nSide-by-side comparison (demo plot)\n</code></pre> <p>The docker build command must be run once to create the Docker image before the data drift experiment can be executed with docker run.</p>"},{"location":"pipeline/data_drift/#build-a-docker-image","title":"Build  a Docker image","text":"<pre><code>docker build -f data_drift_demo.dockerfile -t mlops-data-drift-demo .\n</code></pre> <p>Run the container:</p> <pre><code>docker run --rm `\n  -v \"$((Get-Location).Path)\\data:/app/data\" `\n  -v \"$((Get-Location).Path)\\models:/app/models\" `\n  -v \"$((Get-Location).Path)\\results:/app/results\" `\n  -e COLOR_CONTRAST=0.6 `\n  -e CLASS_NAME=carpet `\n  -e DATA_ROOT=/app/data `\n  -e OUT_ROOT=/app/results/data_drift `\n  mlops-data-drift-demo\n</code></pre>"},{"location":"pipeline/data_drift/#outputs","title":"Outputs","text":"<p>After completion, the following artifacts are produced:</p> <ul> <li>Drifted datasets (train &amp; test)</li> <li>Heatmaps for both runs</li> <li>Histogram before adaptation</li> <li>Histogram after adaptation</li> <li>Comparison plot (demo.py)</li> </ul>"},{"location":"pipeline/evaluation/","title":"Evaluation pipeline","text":"<p>The evaluation script quantitatively assesses the performance of the anomaly detection model on a labeled dataset. It compares predicted anomaly scores against ground-truth labels to compute evaluation metrics, enabling objective comparison of models, hyperparameters, and configurations in a reproducible way. We used logs on evaluation with key metrics such as image-level and pixel-level ROC AUC. Each run produces a timestamped log file in <code>logs/</code> for traceability.</p> <ul> <li>Metric Calculation: Computes Image-Level ROC AUC and, if masks are available, Pixel-Level ROC AUC.</li> <li>Visualization: Generates histograms of anomaly scores (Good vs. Defective) and Pixel ROC curves.</li> <li>Dual Modes: Can run full inference using a model and memory bank, or quickly re-evaluate using pre-computed scores from a JSONL log.</li> </ul>"},{"location":"pipeline/evaluation/#arguments","title":"Arguments","text":"<p>--data_root: Path to the dataset root folder.</p> <p>--class_name: The specific object category to evaluate.</p> <p>--weights_path: Path to the pretrained DINOv3 model weights.</p> <p>--memory_bank_path: Path to the computed memory bank (.pt file).</p> <p>--output_dir: Directory to save results (plots and ROC curves); defaults to ./results.</p> <p>--k: Number of nearest neighbors to use for anomaly scoring (default: 10).</p> <p>--scores_jsonl: (Optional) Path to a JSONL file with pre-computed scores. If provided, skips inference and only generates histograms/metrics.</p> <p>--hist_only: (Optional) Flag to skip pixel-level evaluation for faster execution.</p>"},{"location":"pipeline/evaluation/#full-evaluation","title":"Full evaluation","text":"<p>This command runs the full pipeline: loading the model, computing anomaly maps for the test set, and calculating both image and pixel-level AUC. <pre><code>uv run src/anomaly_detection/evaluate.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain.pth \\\n  --memory_bank_path ./models/checkpoints/memory_bank_carpet.pt \\\n  --output_dir ./results \\\n  --img_size 224 \\\n  --k 10\n</code></pre></p>"},{"location":"pipeline/evaluation/#fast-re-evaluation-from-logs","title":"Fast Re-Evaluation (From Logs)","text":"<p>If you have already run inference and saved scores, use this to quickly regenerate histograms or check Image AUC without reloading the heavy model. <pre><code>uv run src/anomaly_detection/evaluate.py \\\n  --class_name carpet \\\n  --scores_jsonl ./logs/inference_scores.jsonl \\\n  --output_dir ./results_reanalysis\n</code></pre></p>"},{"location":"pipeline/inference/","title":"Inference pipeline","text":"<p>This script runs the anomaly detection pipeline on new images using a pre-trained DINOv3 model and a pre-computed memory bank. It generates visual outputs (heatmaps and overlays) and quantitative data (anomaly scores) without retraining.</p> <ul> <li>Core Function: Loads a frozen DINOv3 model and a memory bank to compute anomaly scores for each image in the dataset.</li> <li>Visualization: Produces pixel-level anomaly heatmaps and overlays them onto the original images to localize defects.</li> <li>Scoring: Calculates a scalar anomaly score for each image (max value of the anomaly map) and can save these scores to a JSONL file for later analysis.</li> </ul>"},{"location":"pipeline/inference/#what-it-does","title":"What it does:","text":"<ol> <li>Loads a test image and the pre-computed memory bank (.pt file).</li> <li>Loads the frozen DINOv3 feature extractor.</li> <li>Computes anomaly maps by comparing image patches to the memory bank (k-NN).</li> <li>Generates and saves visual outputs (heatmaps and overlays).</li> <li>Calculates per-image anomaly scores and logs them to a JSONL file.</li> </ol>"},{"location":"pipeline/inference/#arguments","title":"Arguments","text":"<p>--data_root: Path to the root directory of the dataset.</p> <p>--class_name: The object category to analyze.</p> <p>--weights_path: Path to the pretrained DINOv3 model weights.</p> <p>--memory_bank_path: Path to the saved memory bank (.pt file).</p> <p>--output_dir: Directory where results will be saved; defaults to ./results/figures.</p> <p>--output_name: (Optional) Custom name for the output folder; defaults to the class name.</p> <p>--split: The dataset split to run inference on (train or test); defaults to test.</p> <p>--save_heatmaps: Flag to enable saving of anomaly heatmaps (enabled by default).</p> <p>--save_overlays: Flag to enable saving of heatmaps overlaid on original images (enabled by default).</p> <p>--heatmaps_only: Convenience flag to disable overlays and save only the heatmaps.</p> <p>--scores_jsonl: (Optional) Path to save per-image anomaly scores and labels in JSONL format.</p> <p>--img_size: Target image size for resizing (default: 224).</p> <p>--batch_size: Batch size for data loading (default: 8).</p> <p>--k: Number of nearest neighbors to use for anomaly scoring (default: 10).</p>"},{"location":"pipeline/inference/#standard-inference-visuals-scores","title":"Standard Inference (Visuals + Scores)","text":"<p>This is the most common use case: generating heatmaps, overlays, and a score log for the test set.</p> <pre><code>uv run src/anomaly_detection/inference.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain.pth \\\n  --memory_bank_path ./models/memory_bank.pt \\\n  --output_dir ./results/figures \\\n  --scores_jsonl ./logs/inference_scores.jsonl \\\n  --save_heatmaps \\\n  --save_overlays \\\n  --k 10\n</code></pre>"},{"location":"pipeline/inference/#heatmaps-only-faster-visualization","title":"Heatmaps Only (Faster Visualization)","text":"<p>Use this when you only need to inspect the raw anomaly signal without the context of the original image (saves disk space and time). <pre><code>uv run src/anomaly_detection/inference.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain.pth \\\n  --memory_bank_path ./models/memory_bank.pt \\\n  --heatmaps_only\n</code></pre></p>"},{"location":"pipeline/train/","title":"Training pipeline (memory bank)","text":"<p>In this Zero-Shot framework, \"training\" does not involve gradient updates. Instead, this script builds a Memory Bank of features from normal data.</p> <ul> <li>Feature Extraction: Runs the frozen DINOv3 model on the training set (only \"good\" images) to extract high-level feature representations.</li> <li>Memory Bank Construction: Aggregates these feature vectors into a single tensor, acting as the \"reference\" for normality.</li> <li>Persistence: Saves the resulting tensor (.pt) to disk, which is required by the inference and evaluation modules.</li> </ul>"},{"location":"pipeline/train/#what-it-does","title":"What it does:","text":"<ol> <li>Loads training images (<code>train/good/</code>)</li> <li>Loads DINOv3 weights</li> <li>Extracts patch features</li> <li>Builds a memory bank tensor</li> <li>Saves it as a <code>.pt</code> file</li> </ol>"},{"location":"pipeline/train/#arguments","title":"Arguments","text":"<p>--data_root: Path to the dataset root directory.</p> <p>--class_name: The specific object category to model.</p> <p>--weights_path: Path to the pretrained DINOv3 model weights.</p> <p>--save_path: File path where the memory bank .pt file will be saved. Defaults to ./models/memory_bank.pt.</p> <p>--img_size: Target image size for resizing (default: 224). Must match the size used during inference.</p> <p>--batch_size: Number of images processed at once (default: 8).</p> <p>--augment: (Optional) Enable data augmentation during memory bank creation. Typically False for standard banks, but can be used to build robust banks.</p> <p>--aug_types: (Optional) List of augmentations to apply if --augment is set (e.g., rotation, color, blur).</p>"},{"location":"pipeline/train/#standard-memory-bank-construction","title":"Standard Memory Bank Construction","text":"<p>This is the default usage: it processes the \"good\" training images and creates a standard reference bank.</p> <pre><code>uv run src/anomaly_detection/train.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain.pth \\\n  --save_path ./models/checkpoints/memory_bank_carpet.pt \\\n  --img_size 224 \\\n  --batch_size 16\n</code></pre>"},{"location":"pipeline/train/#robust-memory-bank-with-augmentation","title":"Robust Memory Bank (with Augmentation)","text":"<p>If you expect the test environment to have specific variations (e.g., slight rotations or lighting changes) that should be considered \"normal,\" you can inject them into the memory bank directly.</p> <pre><code>uv run src/anomaly_detection/train.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain.pth \\\n  --save_path ./models/checkpoints/memory_bank_carpet_robust.pt \\\n  --augment \\\n  --aug_types rotation color \\\n  --rot_degrees 5 \\\n  --color_brightness 0.1\n</code></pre>"}]}