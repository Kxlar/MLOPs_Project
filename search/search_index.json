{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Anomaly Detection (MVTec)","text":"<p>This project implements an Zero-shot anomaly detection for industrial inspection using MVTec AD dataset and working with Carpet specifically. It supports training a memory bank, generating anomaly heatmaps, and evaluating results \u2014 locally, in Docker, or on cloud VMs.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide helps you run the anomaly detection project locally as quickly as possible. It assumes no prior knowledge of the codebase.</p>"},{"location":"getting-started/#1-system-requirements","title":"1. System Requirements","text":"<ul> <li>Linux / Windows /   (We recommend that Windows users use WSL)</li> <li>Python \u2265 3.11 (3.13 recommended)</li> <li>Docker</li> <li>Git</li> </ul>"},{"location":"getting-started/#2-installation","title":"2. Installation","text":"<p>Clone the repository: <pre><code>git clone https://github.com/Kxlar/MLOPs_Project.git\n</code></pre></p>"},{"location":"getting-started/#3-data-and-pre-trained-model","title":"3. Data and Pre-trained model","text":"<p>Downloads the data and the pretrained model from GCP bucket</p> <p><pre><code>chmod +x setup.sh\n</code></pre> <pre><code>./setup.sh\n</code></pre></p> <p>Install dependencies: <pre><code>uv sync\n</code></pre> Verify installation: <pre><code>uv run python -c \"import torch; print('OK')\"\n</code></pre></p>"},{"location":"getting-started/#3-project-structure","title":"3. Project Structure","text":"<pre><code>src/anomaly_detection/\n\u251c\u2500\u2500 api.py                # FastAPI entrypoint\n\u251c\u2500\u2500 service.py            # Backend service logic\n\u251c\u2500\u2500 train.py              # Training\n\u251c\u2500\u2500 inference.py          # Inference\n\u251c\u2500\u2500 evaluate.py           # Evaluation\n\u251c\u2500\u2500 hydra/\n\u2502   \u251c\u2500\u2500 train_hydra.py\n\u2502   \u251c\u2500\u2500 inference_hydra.py\n\u2502   \u251c\u2500\u2500 evaluate_hydra.py\n\u2502   \u251c\u2500\u2500 augment_hydra.py\n\u2502   \u2514\u2500\u2500 hydra_utils.py\nconfigs/\n\u251c\u2500\u2500 train.yaml\n\u251c\u2500\u2500 inference.yaml\n\u251c\u2500\u2500 evaluate.yaml\n\u251c\u2500\u2500 augment.yaml\ndockerfiles/\n\u251c\u2500\u2500 train.dockerfile\n\u251c\u2500\u2500 inference.dockerfile\n\u251c\u2500\u2500 evaluation.dockerfile\n\u251c\u2500\u2500 backend.dockerfile\n\u251c\u2500\u2500 frontend.dockerfile\n</code></pre>"},{"location":"getting-started/#invoke-tasks","title":"Invoke tasks","text":"<pre><code>uv run invoke --list\n</code></pre>"},{"location":"getting-started/#4-minimal-end-to-end-run","title":"4. Minimal End-to-End Run","text":""},{"location":"getting-started/#41-data-loading","title":"4.1 Data loading:","text":"<p>The project expects the MVTec AD dataset in the following structure:</p> data.py<pre><code>uv run python src/anomaly_detection/data.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --img_size 224 \\\n  --batch_size 8\n</code></pre>"},{"location":"getting-started/#42-build-the-memory-bank","title":"4.2 Build the memory bank:","text":"train.py<pre><code>uv run python src/anomaly_detection/train.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth \\\n  --save_path ./models/memory_bank.pt \\\n  --img_size 224 \\\n  --batch_size 8\n</code></pre> <p>This step: - extracts features - builds a memory bank - saves model artifacts under models/</p>"},{"location":"getting-started/#43-run-inference","title":"4.3 Run inference:","text":"inference.py<pre><code>uv run python src/anomaly_detection/inference.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth \\\n  --memory_bank_path ./models/memory_bank.pt \\\n  --output_dir ./results/figures \\\n  --img_size 224 \\\n  --batch_size 8 \\\n  --k 1\n</code></pre> <p>Outputs include: - anomaly heatmaps - anomaly scores</p>"},{"location":"getting-started/#44-evaluate-the-model","title":"4.4 Evaluate the model:","text":"evaluate.py<pre><code>uv run python src/anomaly_detection/evaluate.py \\\n--data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth \\\n  --memory_bank_path ./models/memory_bank.pt \\\n  --output_dir ./results/figures/eval \\\n  --img_size 224 \\\n  --batch_size 8 \\\n  --k 10\n</code></pre> <p>Evaluation outputs are written to: - results/figures/eval</p>"},{"location":"getting-started/#5-run-the-api","title":"5. Run the API","text":"<p>To start the FastAPI backend locally: <pre><code>uv run python src/anomaly_detection/API/api.py\n</code></pre></p> <p>Open in browser: <pre><code>http://localhost:8000\n</code></pre></p> <p>Health check: <pre><code>curl http://localhost:8000/health\n</code></pre></p>"},{"location":"getting-started/#6-run-with-docker","title":"6. Run with Docker","text":"<p>Build and run the backend API using Docker: <pre><code>docker build -f dockerfiles/backend.dockerfile -t anomaly-backend .\ndocker run -p 8000:8000 anomaly-backend\n</code></pre></p>"},{"location":"getting-started/#7-run-bentoml-api","title":"7. Run BentoML API","text":"<p>Build and run the backend API using BentoML.</p> <p>Convert our model into a ONNX model: <pre><code>uv run export_onnx_paranoid.py\n</code></pre></p> <p>Initialize BentoML: <pre><code>uv run bentoml --version\n</code></pre></p> <p>Build the docker container: <pre><code>uv run bentoml build\nbentoml containerize anomaly_detection_service:latest\n</code></pre></p> <p>Copy the container tag and run the following command to launch the API: <pre><code>docker run --rm -p 3000:3000 anomaly_detection_service:TAG\n</code></pre></p>"},{"location":"my_api/","title":"API Reference","text":"<p>This page is automatically generated from the project\u2019s Python docstrings using mkdocstrings.</p>"},{"location":"my_api/#data-preprocessing","title":"Data preprocessing","text":""},{"location":"my_api/#anomaly_detection.data","title":"anomaly_detection.data","text":""},{"location":"my_api/#anomaly_detection.data.build_transform","title":"build_transform","text":"<pre><code>build_transform(img_size: int, args=None, is_saving=False)\n</code></pre> <p>Builds the transform pipeline. Args:     is_saving (bool): If True, returns PIL image (no normalization/tensor conversion).</p> Source code in <code>src/anomaly_detection/data.py</code> <pre><code>def build_transform(img_size: int, args=None, is_saving=False):\n    \"\"\"\n    Builds the transform pipeline.\n    Args:\n        is_saving (bool): If True, returns PIL image (no normalization/tensor conversion).\n    \"\"\"\n    transforms = [T.Resize((img_size, img_size))]\n\n    # Apply augmentations only if requested\n    if args and args.augment and hasattr(args, \"aug_types\"):\n        if \"rotation\" in args.aug_types:\n            transforms.append(T.RandomRotation(degrees=args.rot_degrees))\n\n        if \"color\" in args.aug_types:\n            transforms.append(\n                T.ColorJitter(\n                    brightness=args.color_brightness,\n                    contrast=args.color_contrast,\n                    saturation=args.color_saturation,\n                )\n            )\n\n        if \"blur\" in args.aug_types:\n            transforms.append(T.GaussianBlur(kernel_size=args.blur_kernel))\n\n    # Add Normalization unless we are saving images to disk\n    if not is_saving:\n        transforms.append(T.ToTensor())\n        transforms.append(\n            T.Normalize(\n                mean=(0.485, 0.456, 0.406),  # ImageNet stats\n                std=(0.229, 0.224, 0.225),\n            )\n        )\n\n    return T.Compose(transforms)\n</code></pre>"},{"location":"my_api/#anomaly_detection.data.save_augmented_dataset","title":"save_augmented_dataset","text":"<pre><code>save_augmented_dataset(args)\n</code></pre> <p>Generate and save an augmented (\"drifted\") dataset to disk.</p> <p>This is used by the data drift demo to write a new dataset folder under <code>save_aug_path/save_aug_dataset_name/&lt;split&gt;/...</code>.</p> <ul> <li>For split=train, only <code>good/</code> images are used.</li> <li>For split=test, images are saved under their defect-type folder.   If <code>--include_anomalies</code> is not set, only <code>good/</code> images are drifted.</li> </ul> <p>Note: If you use geometric transforms (e.g. rotation), ground-truth masks are NOT transformed here.</p> Source code in <code>src/anomaly_detection/data.py</code> <pre><code>def save_augmented_dataset(args):\n    \"\"\"Generate and save an augmented (\"drifted\") dataset to disk.\n\n    This is used by the data drift demo to write a new dataset folder under\n    `save_aug_path/save_aug_dataset_name/&lt;split&gt;/...`.\n\n    - For split=train, only `good/` images are used.\n    - For split=test, images are saved under their defect-type folder.\n      If `--include_anomalies` is not set, only `good/` images are drifted.\n\n    Note: If you use geometric transforms (e.g. rotation), ground-truth masks are\n    NOT transformed here.\n    \"\"\"\n\n    if not args.save_aug_path:\n        raise ValueError(\"save_aug_path must be set to save an augmented dataset\")\n\n    if args.split not in {\"train\", \"test\"}:\n        raise ValueError(\"split must be one of: train, test\")\n\n    save_root = Path(args.save_aug_path)\n    dataset_name = args.save_aug_dataset_name or f\"{args.class_name}_augmented\"\n\n    print(f\"Saving augmented dataset to: {save_root} (dataset={dataset_name}, split={args.split})\")\n\n    # Transform: Resize + Augment (No Tensor/Norm)\n    aug_transform = build_transform(args.img_size, args=args, is_saving=True)\n\n    # Collect input images\n    pattern = Path(args.data_root) / args.class_name / args.split / \"*\" / \"*.*\"\n    input_paths = sorted(glob(str(pattern)))\n    if args.split == \"train\":\n        input_paths = [p for p in input_paths if \"good\" in Path(p).parts]\n    elif not args.include_anomalies:\n        input_paths = [p for p in input_paths if \"good\" in Path(p).parts]\n\n    out_base = save_root / dataset_name / args.split\n    out_base.mkdir(parents=True, exist_ok=True)\n\n    count = 0\n    for img_path in input_paths:\n        img_path = Path(img_path)\n        original_img = Image.open(img_path).convert(\"RGB\")\n\n        # MVTec convention: root/class/split/&lt;defect_type&gt;/&lt;filename&gt;\n        defect_type = img_path.parent.name\n        out_dir = out_base / defect_type\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        for i in range(args.aug_multiplier):\n            aug_img = aug_transform(original_img)\n\n            stem = img_path.stem\n            if args.aug_multiplier == 1:\n                save_name = f\"{stem}.png\"\n            else:\n                save_name = f\"{stem}_aug_{i}.png\"\n\n            aug_img.save(out_dir / save_name)\n            count += 1\n\n    print(f\"Process complete. Saved {count} images to {out_base}.\")\n</code></pre>"},{"location":"my_api/#training","title":"Training","text":""},{"location":"my_api/#anomaly_detection.train","title":"anomaly_detection.train","text":""},{"location":"my_api/#anomaly_detection.train.run","title":"run","text":"<pre><code>run(args) -&gt; None\n</code></pre> <p>Build and save the memory bank.</p> <p>Kept as a separate function so Hydra can call the same logic.</p> Source code in <code>src/anomaly_detection/train.py</code> <pre><code>def run(args) -&gt; None:\n    \"\"\"Build and save the memory bank.\n\n    Kept as a separate function so Hydra can call the same logic.\n    \"\"\"\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n\n    # 1. Load Data\n    # For memory bank construction, we typically use standard train set (no huge augmentation needed mostly)\n    train_ds, _, train_loader, _ = build_dataloaders(args)\n    print(f\"Training images: {len(train_ds)}\")\n\n    # 2. Load Model\n    dinov3_model = load_dinov3(args.weights_path, device)\n    feature_extractor = DINOv3FeatureExtractor(dinov3_model).eval().to(device)\n\n    # 3. Build Memory Bank\n    print(\"Building memory bank...\")\n    memory_bank = build_memory_bank(feature_extractor, train_loader, device)\n    print(f\"Memory Bank shape: {memory_bank.shape}\")\n\n    # 4. Save\n    save_path = Path(args.save_path)\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(memory_bank, save_path)\n    print(f\"Memory bank saved to {save_path}\")\n</code></pre>"},{"location":"my_api/#inference","title":"Inference","text":""},{"location":"my_api/#anomaly_detection.inference","title":"anomaly_detection.inference","text":""},{"location":"my_api/#anomaly_detection.inference.run","title":"run","text":"<pre><code>run(args) -&gt; None\n</code></pre> <p>Run inference and save heatmaps/overlays.</p> <p>Kept as a separate function so Hydra can call the same logic.</p> Source code in <code>src/anomaly_detection/inference.py</code> <pre><code>def run(args) -&gt; None:\n    \"\"\"Run inference and save heatmaps/overlays.\n\n    Kept as a separate function so Hydra can call the same logic.\n    \"\"\"\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    if args.heatmaps_only:\n        args.save_overlays = False\n\n    # Setup Paths\n    results_root = Path(args.output_dir) / (args.output_name or args.class_name)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    heatmap_dir = results_root / \"heatmaps\"\n    overlay_dir = results_root / \"overlays\"\n    if args.save_heatmaps:\n        heatmap_dir.mkdir(parents=True, exist_ok=True)\n    if args.save_overlays:\n        overlay_dir.mkdir(parents=True, exist_ok=True)\n\n    scores_f = None\n    if args.scores_jsonl:\n        scores_path = Path(args.scores_jsonl)\n    else:\n        scores_path = None\n\n    # 1. Load Data\n    transform = build_transform(args.img_size, args=None, is_saving=False)\n    dataset = MVTecDataset(args.data_root, args.class_name, split=args.split, transform=transform)\n    print(f\"{args.split.capitalize()} images: {len(dataset)}\")\n\n    # 2. Load Model &amp; Memory Bank\n    dinov3_model = load_dinov3(args.weights_path, device)\n    feature_extractor = DINOv3FeatureExtractor(dinov3_model).eval().to(device)\n\n    print(f\"Loading memory bank from {args.memory_bank_path}...\")\n    memory_bank = torch.load(args.memory_bank_path, map_location=device)\n\n    # 3. Inference Loop\n    print(\"Starting inference...\")\n    if scores_path is not None:\n        scores_path.parent.mkdir(parents=True, exist_ok=True)\n        scores_f = open(scores_path, \"w\", encoding=\"utf-8\")\n\n    try:\n        for i in range(len(dataset)):\n            img_t, label, path = dataset[i]\n\n            anomaly_map = compute_anomaly_map(img_t, feature_extractor, memory_bank, k=args.k)\n            anomaly_score = reduce_anomaly_map(anomaly_map, mode=\"max\")\n            am_up = upsample_anomaly_map(anomaly_map, args.img_size)\n\n            img_path = Path(path)\n            base_name = img_path.stem\n            label_str = \"defect\" if label == 1 else \"good\"\n\n            if scores_f is not None:\n                import json\n\n                scores_f.write(\n                    json.dumps(\n                        {\n                            \"path\": str(img_path.as_posix()),\n                            \"label\": int(label),\n                            \"anomaly_score\": float(anomaly_score),\n                        }\n                    )\n                    + \"\\n\"\n                )\n\n            if args.save_heatmaps or args.save_overlays:\n                out_heatmap = heatmap_dir / f\"{base_name}_{label_str}.png\" if args.save_heatmaps else None\n                out_overlay = overlay_dir / f\"{base_name}_{label_str}.png\" if args.save_overlays else None\n\n                if out_heatmap is None:\n                    # Still need a path for the helper, but don't write it\n                    out_heatmap = heatmap_dir / f\"{base_name}_{label_str}.png\"\n                if out_overlay is None:\n                    out_overlay = overlay_dir / f\"{base_name}_{label_str}.png\"\n\n                if not args.save_heatmaps and not args.save_overlays:\n                    pass\n                elif args.save_heatmaps and args.save_overlays:\n                    save_heatmap_and_overlay(\n                        str(img_path),\n                        am_up,\n                        args.img_size,\n                        out_heatmap,\n                        out_overlay,\n                    )\n                elif args.save_heatmaps and not args.save_overlays:\n                    # Save only heatmap\n                    plt.figure()\n                    plt.axis(\"off\")\n                    am_norm = (am_up - am_up.min()) / (am_up.max() - am_up.min() + 1e-8)\n                    plt.imshow(am_norm, cmap=\"jet\")\n                    plt.tight_layout(pad=0)\n                    plt.savefig(out_heatmap, bbox_inches=\"tight\", pad_inches=0)\n                    plt.close()\n                elif not args.save_heatmaps and args.save_overlays:\n                    # Save only overlay\n                    plt.figure()\n                    plt.axis(\"off\")\n                    am_norm = (am_up - am_up.min()) / (am_up.max() - am_up.min() + 1e-8)\n                    orig_img = Image.open(img_path).convert(\"RGB\").resize((args.img_size, args.img_size))\n                    orig_np = np.array(orig_img)\n                    plt.imshow(orig_np)\n                    plt.imshow(am_norm, cmap=\"jet\", alpha=0.5)\n                    plt.tight_layout(pad=0)\n                    plt.savefig(out_overlay, bbox_inches=\"tight\", pad_inches=0)\n                    plt.close()\n\n            if i % 10 == 0:\n                print(f\"Processed {i}/{len(dataset)}\")\n    finally:\n        if scores_f is not None:\n            scores_f.close()\n\n    print(f\"Done. Results saved in {results_root}\")\n    if scores_path is not None:\n        print(f\"Scores saved in {scores_path}\")\n</code></pre>"},{"location":"my_api/#evaluation","title":"Evaluation","text":""},{"location":"my_api/#anomaly_detection.evaluate","title":"anomaly_detection.evaluate","text":""},{"location":"my_api/#anomaly_detection.evaluate.run","title":"run","text":"<pre><code>run(args) -&gt; None\n</code></pre> <p>Evaluate image-level and pixel-level ROC/AUC.</p> <p>Kept as a separate function so Hydra can call the same logic.</p> Source code in <code>src/anomaly_detection/evaluate.py</code> <pre><code>def run(args) -&gt; None:\n    \"\"\"Evaluate image-level and pixel-level ROC/AUC.\n\n    Kept as a separate function so Hydra can call the same logic.\n    \"\"\"\n\n    roc_dir = Path(args.output_dir) / args.class_name / \"roc\"\n    roc_dir.mkdir(parents=True, exist_ok=True)\n\n    if args.scores_jsonl is not None:\n        import json\n\n        y_true = []\n        y_score_max = []\n        with open(args.scores_jsonl, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                rec = json.loads(line)\n                if rec.get(\"anomaly_score\") is None or rec.get(\"label\") is None:\n                    continue\n                y_true.append(int(rec[\"label\"]))\n                y_score_max.append(float(rec[\"anomaly_score\"]))\n\n        y_true = np.asarray(y_true)\n        y_score_max = np.asarray(y_score_max)\n        if len(y_true) == 0:\n            raise ValueError(f\"No usable records found in scores file: {args.scores_jsonl}\")\n\n        # Compute AUC if both classes are present\n        if len(np.unique(y_true)) &gt; 1:\n            auc_val = roc_auc_score(y_true, y_score_max)\n            print(f\"Image-Level ROC AUC (from scores): {auc_val:.4f}\")\n        else:\n            print(\"Only one class present in scores; skipping AUC.\")\n\n        # Plot Histogram\n        plt.figure()\n        plt.hist(y_score_max[y_true == 0], bins=20, alpha=0.6, label=\"Good\")\n        plt.hist(y_score_max[y_true == 1], bins=20, alpha=0.6, label=\"Defective\")\n        plt.legend()\n        plt.title(f\"{args.class_name} - Anomaly Score Histogram\")\n        plt.savefig(roc_dir / \"histogram.png\")\n        plt.close()\n\n        print(f\"Saved histogram to {roc_dir / 'histogram.png'}\")\n        return\n\n    if args.data_root is None or args.weights_path is None or args.memory_bank_path is None:\n        raise ValueError(\n            \"When --scores_jsonl is not provided, --data_root/--weights_path/--memory_bank_path are required\"\n        )\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 1. Load Data\n    _, test_dataset, _, _ = build_dataloaders(args)\n\n    # 2. Load Model &amp; Memory Bank\n    dinov3_model = load_dinov3(args.weights_path, device)\n    feature_extractor = DINOv3FeatureExtractor(dinov3_model).eval().to(device)\n    memory_bank = torch.load(args.memory_bank_path, map_location=device)\n\n    # --- Image Level Evaluation ---\n    print(\"Running Image-Level Evaluation...\")\n    y_true = []\n    y_score_max = []\n\n    for i in range(len(test_dataset)):\n        img_t, label, _ = test_dataset[i]\n        anomaly_map = compute_anomaly_map(img_t, feature_extractor, memory_bank, k=args.k)\n\n        y_true.append(label)\n        y_score_max.append(reduce_anomaly_map(anomaly_map, mode=\"max\"))\n\n    y_true = np.array(y_true)\n    y_score_max = np.array(y_score_max)\n\n    auc_val = roc_auc_score(y_true, y_score_max)\n    print(f\"Image-Level ROC AUC: {auc_val:.4f}\")\n\n    # Plot Histogram\n    plt.figure()\n    plt.hist(y_score_max[y_true == 0], bins=20, alpha=0.6, label=\"Good\")\n    plt.hist(y_score_max[y_true == 1], bins=20, alpha=0.6, label=\"Defective\")\n    plt.legend()\n    plt.title(f\"{args.class_name} - Anomaly Score Histogram\")\n    plt.savefig(roc_dir / \"histogram.png\")\n    plt.close()\n\n    # --- Pixel Level Evaluation ---\n    print(\"Running Pixel-Level Evaluation...\")\n    if args.hist_only:\n        print(\"hist_only enabled; skipping pixel evaluation.\")\n        return\n\n    pixel_y_true = []\n    pixel_scores = []\n\n    for i in range(len(test_dataset)):\n        img_t, label, path = test_dataset[i]\n\n        gt_mask = load_ground_truth_mask(path, args.data_root, args.class_name, args.img_size)\n        if gt_mask is None:\n            if label == 0:\n                gt_mask = np.zeros((args.img_size, args.img_size), dtype=np.uint8)\n            else:\n                continue\n\n        anomaly_map = compute_anomaly_map(img_t, feature_extractor, memory_bank, k=args.k)\n        am_up = upsample_anomaly_map(anomaly_map, args.img_size)\n\n        pixel_y_true.append(gt_mask.flatten())\n        pixel_scores.append(am_up.flatten())\n\n    if pixel_y_true:\n        pixel_y_true = np.concatenate(pixel_y_true)\n        pixel_scores = np.concatenate(pixel_scores)\n\n        pixel_auc = roc_auc_score(pixel_y_true, pixel_scores)\n        print(f\"Pixel-Level ROC AUC: {pixel_auc:.4f}\")\n\n        fpr, tpr, _ = roc_curve(pixel_y_true, pixel_scores)\n        plt.figure()\n        plt.plot(fpr, tpr, label=f\"AUC = {pixel_auc:.4f}\")\n        plt.plot([0, 1], [0, 1], linestyle=\"--\")\n        plt.title(f\"{args.class_name} - Pixel ROC\")\n        plt.legend()\n        plt.savefig(roc_dir / \"pixel_roc.png\")\n        plt.close()\n    else:\n        print(\"No masks found, skipping pixel evaluation.\")\n</code></pre>"},{"location":"my_api/#api-fastapi","title":"API (FastAPI)","text":""},{"location":"my_api/#anomaly_detection.API.api","title":"anomaly_detection.API.api","text":""},{"location":"my_api/#anomaly_detection.API.api.health_check","title":"health_check","text":"<pre><code>health_check()\n</code></pre> <p>Simple health check to verify the API is running and models are loaded.</p> Source code in <code>src/anomaly_detection/API/api.py</code> <pre><code>@app.get(\"/health\")\ndef health_check():\n    \"\"\"\n    Simple health check to verify the API is running and models are loaded.\n    \"\"\"\n    return {\n        \"status\": \"ok\",\n        \"device\": str(config.device),\n        \"models_loaded\": \"feature_extractor\" in ml_models and \"memory_bank\" in ml_models,\n    }\n</code></pre>"},{"location":"my_api/#anomaly_detection.API.api.lifespan","title":"lifespan  <code>async</code>","text":"<pre><code>lifespan(app: FastAPI)\n</code></pre> <p>Handles model loading and optional auto-training on startup.</p> Source code in <code>src/anomaly_detection/API/api.py</code> <pre><code>@contextlib.asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Handles model loading and optional auto-training on startup.\n    \"\"\"\n    device = config.device\n    print(f\"--- Starting API for class: {config.class_name} on {device} ---\")\n\n    # 1. Load Backbone (DINOv3)\n    if not Path(config.weights_path).exists():\n        print(f\"[!] Error: Weights file not found at {config.weights_path}\")\n        raise FileNotFoundError(\"DINOv3 weights missing.\")\n\n    dinov3 = load_dinov3(config.weights_path, device)\n    feature_extractor = DINOv3FeatureExtractor(dinov3).eval().to(device)\n    ml_models[\"feature_extractor\"] = feature_extractor\n\n    # 2. Load or Build Memory Bank\n    mb_path = Path(config.memory_bank_path)\n\n    if mb_path.exists():\n        print(f\"[-] Loading memory bank from {mb_path}...\")\n        memory_bank = torch.load(mb_path, map_location=device)\n    else:\n        # TRIGGER AUTO-BUILD\n        memory_bank = run_auto_build(config, feature_extractor, device)\n\n    ml_models[\"memory_bank\"] = memory_bank\n    print(\"--- System Ready ---\")\n\n    yield\n\n    ml_models.clear()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"my_api/#anomaly_detection.API.api.run_auto_build","title":"run_auto_build","text":"<pre><code>run_auto_build(cfg: APIConfig, feature_extractor, device)\n</code></pre> <p>Automatically builds the memory bank if it is missing. Mimics the logic from train.py.</p> Source code in <code>src/anomaly_detection/API/api.py</code> <pre><code>def run_auto_build(cfg: APIConfig, feature_extractor, device):\n    \"\"\"\n    Automatically builds the memory bank if it is missing.\n    Mimics the logic from train.py.\n    \"\"\"\n    print(f\"[-] Memory bank not found at {cfg.memory_bank_path}\")\n    print(\"[-] Starting automatic build process...\")\n\n    mock_args = SimpleNamespace(\n        data_root=cfg.data_root,\n        class_name=cfg.class_name,\n        img_size=cfg.img_size,\n        batch_size=cfg.batch_size,\n        augment=False,\n        aug_types=[],\n    )\n\n    try:\n        # 1. Load Data\n        print(\"    Loading training data...\")\n        _, _, train_loader, _ = build_dataloaders(mock_args)\n\n        # 2. Build Bank\n        print(\"    Extracting features (this may take a while)...\")\n        memory_bank = build_memory_bank(feature_extractor, train_loader, device)\n        print(f\"    Memory Bank shape: {memory_bank.shape}\")\n\n        # 3. Save\n        save_path = Path(cfg.memory_bank_path)\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        torch.save(memory_bank, save_path)\n        print(f\"[-] Memory bank saved to {save_path}\")\n\n        return memory_bank\n\n    except Exception as e:\n        print(f\"[!] Critical Error during auto-build: {e}\")\n        raise RuntimeError(\"Failed to auto-build memory bank. Check dataset path.\") from e\n</code></pre>"},{"location":"deployment/cloud/","title":"Cloud (GCP)","text":"<p>The application was deployed on Google Cloud Run using a two-service architecture, separating the frontend and the backend components.</p> <p>The frontend service provides the user interface and is exposed through a public URL automatically generated by Cloud Run:</p> <p><pre><code>https://frontend-445436263618.europe-west1.run.app/\n</code></pre> which is used by users to interact with the application. The final version works with the BentoML backend service, as it provides a robust and scalable solution for serving the machine learning model. The frontend communicates with the backend through its Cloud Run endpoint, enabling a clean separation of concerns and independent monitoring of each service.</p>"},{"location":"deployment/docker/","title":"Docker Setup","text":"<p>This project uses multiple Dockerfiles, each designed for a specific stage of the MLOps workflow: training, inference, evaluation, backend API serving, frontend UI, and an end-to-end data-drift demo.</p> <p>All containers are reproducible and lockfile-driven, primarily using uv with uv.lock and pyproject.toml.</p> Dockerfile Purpose Typical Usage <code>backend.dockerfile</code> FastAPI backend (API server) Local &amp; cloud inference <code>frontend.dockerfile</code> Streamlit UI Local &amp; Cloud Run <code>train.dockerfile</code> Model training Local / CI / cloud jobs <code>evaluation.dockerfile</code> Model evaluation Reproducible metrics <code>inference.dockerfile</code> Batch inference Heatmaps &amp; overlays <code>data_drift_demo.dockerfile</code> End-to-end drift demo Experimental pipeline <p>Note</p> <p>Since these files are not named <code>Dockerfile</code>, you must always use <code>-f</code> when building.</p>"},{"location":"deployment/docker/#1-backend-api-fastapi-backenddockerfile","title":"1. Backend API (FastAPI) \u2014 <code>backend.dockerfile</code>","text":""},{"location":"deployment/docker/#build-run","title":"Build &amp; run","text":"<p><pre><code>docker build -f backend.dockerfile -t anomaly-backend .\ndocker run --rm -p 8000:8000 anomaly-backend\n</code></pre> Entrypoint</p> <pre><code>uv run uvicorn src.anomaly_detection.api:app --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"deployment/docker/#2-backend-api-bento","title":"2. Backend API (Bento)","text":"<p>We experimented with a BentoML backend but ultimately settled on FastAPI for simplicity and consistency with the rest of the stack.</p> <p>For BentoML app to do inference:</p> <ol> <li>Initialize bentoml <pre><code>uv run bentoml --version\n</code></pre></li> <li>Convert the torch model into a ONNX model <pre><code>uv run export_onnx_paranoid.py\n</code></pre></li> <li>Build the bento container <pre><code>uv run bentoml build\n</code></pre></li> <li>Launch docker in the background. bBuild the docker container based on the bento - replace TAG by the number that shows up after step 3 <pre><code>bentoml containerize anomaly_detection_service:&lt;TAG&gt;\n</code></pre></li> <li> <p>Run the container <pre><code>docker run --rm -p 3000:3000 anomaly_detection_service:&lt;TAG&gt;\n</code></pre></p> </li> <li> <p>To use the backend, go to http://localhost:3000 </p> </li> </ol> <p>OR (once the docker image is running) in new terminal run: <pre><code>uv run src/anomaly_detection/api_inference.py\n</code></pre></p>"},{"location":"deployment/docker/#3-frontend-ui-streamlit-frontenddockerfile","title":"3. Frontend UI (Streamlit)  \u2014 <code>frontend.dockerfile</code>","text":"<p>Runs a Streamlit web interface for uploading images and calling the backend API. Designed to work both locally and on Google Cloud Run (uses $PORT).</p>"},{"location":"deployment/docker/#build-run-local","title":"Build &amp; run (local)","text":"<pre><code>docker build -f frontend.dockerfile -t anomaly-frontend .\ndocker run --rm -p 8080:8080 -e PORT=8080 anomaly-frontend\n</code></pre>"},{"location":"deployment/docker/#4-training-traindockerfile","title":"4. Training \u2014 <code>train.dockerfile</code>","text":"<p>Runs the training pipeline in a fully reproducible container.</p>"},{"location":"deployment/docker/#build-run_1","title":"Build &amp; run","text":"<pre><code>docker build -f train.dockerfile -t anomaly-train .\ndocker run --rm anomaly-train\n</code></pre>"},{"location":"deployment/docker/#5-evaluation-evaluationdockerfile","title":"5. Evaluation  \u2014 <code>evaluation.dockerfile</code>","text":"<p>Runs model evaluation on a dataset to compute metrics in a controlled and reproducible environment.</p>"},{"location":"deployment/docker/#build-run_2","title":"Build &amp; run","text":"<pre><code>docker build -f evaluation.dockerfile -t anomaly-eval .\ndocker run --rm anomaly-eval\n</code></pre>"},{"location":"deployment/docker/#6-inference-inferencedockerfile","title":"6. Inference \u2014 <code>inference.dockerfile</code>","text":"<p>Runs offline inference tasks such as heatmap generation, overlay visualization and batch prediction.</p>"},{"location":"deployment/docker/#build-run_3","title":"Build &amp; run","text":"<pre><code>docker build -f inference.dockerfile -t anomaly-infer .\ndocker run --rm anomaly-infer\n</code></pre>"},{"location":"deployment/docker/#7-data-drifting-demo-data_drift_demodockerfile","title":"7. Data drifting Demo \u2014 <code>data_drift_demo.dockerfile</code>","text":"<p>Runs an end-to-end data drift experiment at container runtime via an entrypoint script (not during image build). Typical steps include generating drifted datasets, running the API, calling it, and producing plots.</p> <p>Building and running the docker for data drifting is shown in section Pipeline/ Data Drift.</p>"},{"location":"deployment/frontend/","title":"Frontend (Streamlit UI)","text":"<p>This project includes a lightweight Streamlit frontend that allows users to upload an image and run anomaly detection through the backend API.</p> <p>The frontend does not perform inference itself \u2014 it acts as a client that sends images to the FastAPI backend and displays the returned prediction.</p>"},{"location":"deployment/frontend/#overview","title":"Overview","text":"<p>The frontend provides:</p> <ul> <li>Image upload (<code>.png</code>, <code>.jpg</code>, <code>.jpeg</code>)</li> <li>One-click inference</li> <li>Visualization of:</li> <li>Uploaded image</li> <li>Anomaly score</li> <li>Prediction result</li> <li>Clear separation between UI (frontend) and inference (backend)</li> </ul>"},{"location":"deployment/frontend/#architecture","title":"Architecture:","text":"<pre><code>User Browser\n     \u2193\nStreamlit Frontend (port 8501)\n     \u2193  HTTP POST /predict\nFastAPI Backend (port 8000)\n     \u2193\nDINOv3 + Memory Bank\n</code></pre>"},{"location":"deployment/frontend/#prerequisites","title":"Prerequisites","text":"<p>Before running the frontend, make sure:</p> <ol> <li>You have built a memory bank using <code>train.py</code></li> <li>The FastAPI backend is running</li> <li>Dependencies are installed via <code>uv</code></li> </ol>"},{"location":"deployment/frontend/#1-running-the-backend-required","title":"1. Running the Backend (Required)","text":"<p>The frontend expects the backend to be available at: <pre><code>http://127.0.0.1:8000\n</code></pre></p> <p>Start the backend in a dedicated terminal and keep it running:</p> <pre><code>cd ~/projects/MLOPs_Project\nuv run uvicorn src.anomaly_detection.api:app --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"deployment/frontend/#2-running-the-frontend-streamlit","title":"2. Running the Frontend (Streamlit)","text":"<p>Open a second terminal and run:</p> <pre><code>cd ~/projects/MLOPs_Project\nuv run streamlit run frontend/frontend.py \\\n  --server.port 8501 \\\n  --server.address 0.0.0.0\n</code></pre> <p>You should see:</p> <p>You can now view your Streamlit app in your browser. URL: http://0.0.0.0:8501</p> <p>Open in your browser: <pre><code>http://localhost:8501\n</code></pre></p>"},{"location":"deployment/frontend/#using-the-frontend","title":"Using the Frontend","text":"<ol> <li> <p>Open the Streamlit page</p> </li> <li> <p>Upload an image from the MVTec dataset (or a compatible image)</p> </li> <li> <p>Click Run prediction</p> </li> </ol> <p>The frontend will:</p> <ul> <li> <p>Send the image to /predict</p> </li> <li> <p>Wait for the backend response</p> </li> <li> <p>Display prediction results</p> </li> </ul> <p>If the backend is not running, you will see a connection error.</p>"},{"location":"development/hydra/","title":"Hydra","text":""},{"location":"development/hydra/#run-hydra-scripts","title":"Run (Hydra scripts)","text":"<pre><code>uv sync\nuv run python -m anomaly_detection.hydra.train_hydra\nuv run python -m anomaly_detection.hydra.evaluate_hydra\nuv run python -m anomaly_detection.hydra.inference_hydra\nuv run python -m anomaly_detection.hydra.augment_hydra\n</code></pre>"},{"location":"development/hydra/#override-parameters-example","title":"Override parameters (example)","text":"<pre><code>uv run src/anomaly_detection/evaluate_hydra.py class_name=carpet k=5 output_dir=./results_k5\n</code></pre>"},{"location":"development/logging-profiling/","title":"Profiling","text":"<p>Profiler used: cProfile</p> <p>Target script:  evaluate.py</p> <p>The evaluation script was profiled using Python\u2019s built-in cProfile, sorted by cumulative runtime:</p> <pre><code>rm -f evaluate.prof\n\nuv run python -m cProfile -s cumulative -o evaluate.prof \\\n  src/anomaly_detection/evaluate.py \\\n  --data_root ./data/raw \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth \\\n  --memory_bank_path ./models/memory_bank.pt \\\n  --output_dir ./reports/figures/eval \\\n  --img_size 224 \\\n  --batch_size 8 \\\n  --k 10\n</code></pre> <p>To inspect the most time-consuming functions:</p> <pre><code>uv run python -c \"import pstats; p=pstats.Stats('evaluate.prof'); p.sort_stats('cumulative').print_stats(20)\"\n</code></pre> <p>Visualization The profiling results were visualized using SnakeViz:</p> <pre><code>uv add snakeviz\nuv run snakeviz evaluate.prof\n</code></pre>"},{"location":"development/logging-profiling/#logging","title":"Logging","text":"<p>Training and evaluation scripts use Loguru for lightweight logging.</p> <ul> <li>Logs are printed to the terminal</li> <li>Logs are also written to <code>logs/</code></li> <li>Log files are timestamped per run</li> </ul> <p>This helps with debugging and reproducibility without cluttering the codebase.</p> <p>Evaluation logs key metrics such as image-level and pixel-level ROC AUC. Each run produces a timestamped log file in <code>logs/</code> for traceability.</p>"},{"location":"development/style/","title":"Style &amp; Linting","text":""},{"location":"development/style/#tools","title":"Tools","text":"<ul> <li>Ruff: Fast Python linter and formatter.</li> <li>pre-commit: Runs checks automatically at commit/push.</li> <li>Pytest + Coverage: Executed by a pre-commit hook to keep tests green.</li> </ul> <p>Configuration lives in <code>pyproject.toml</code> and <code>.pre-commit-config.yaml</code>.</p>"},{"location":"development/style/#format","title":"Format","text":"<ul> <li>Check formatting (no changes): <pre><code>uv run ruff format --check .e\n</code></pre></li> <li>Auto-format (apply changes): <pre><code>uv run ruff format .\n</code></pre></li> </ul>"},{"location":"development/style/#lint","title":"Lint","text":"<ul> <li>Lint entire repo: <pre><code>uv run ruff check .\n</code></pre></li> <li>Apply autofixes for fixable lint rules: <pre><code>uv run ruff check . --fix\n</code></pre></li> </ul>"},{"location":"development/style/#pre-commit-hooks-on-commitpush","title":"pre-commit Hooks (on commit/push)","text":"<p>Hooks are defined in .pre-commit-config.yaml. They include: - General hygiene: trailing whitespace, end-of-file, YAML check, large files - Ruff lint with <code>--fix</code> and <code>ruff-format</code> - Unit tests via <code>pytest</code></p> <p>Install hooks so they run automatically on <code>git commit</code>: <pre><code>uv run pre-commit install\n</code></pre></p>"},{"location":"development/tests/","title":"Tests","text":"<p>uv run coverage run -m pytest .\\tests\\ uv run coverage report -m</p>"},{"location":"pipeline/data/","title":"Data","text":"<p>This project uses the MVTec AD dataset.</p>"},{"location":"pipeline/data/#1-downloading-dataset","title":"1. Downloading Dataset","text":"<p>Our dataset can be downloaded from the cloud</p> <p><pre><code>chmod +x setup.sh\n</code></pre> <pre><code>./setup.sh\n</code></pre></p>"},{"location":"pipeline/data/#2-data-module-documentation","title":"2. Data Module Documentation","text":"<p>The data.py script handles data loading, preprocessing, and synthetic data generation (drift simulation). It manages the MVTec AD dataset for both training (normal only) and testing (normal + anomalies).</p> <ul> <li>Dataset Handling: Manages train/test splits, ensuring only \"good\" data is used for training while the test set includes anomalies with binary labels.</li> <li>Preprocessing: Applies resizing and normalization using ImageNet statistics.</li> <li>Augmentation Pipeline: Supports on-the-fly or offline augmentations (Rotation, Color Jitter, Blur) to simulate data drift or increase robustness.</li> </ul>"},{"location":"pipeline/data/#what-it-does","title":"What it does:","text":"<ol> <li>Scans the file system to load MVTec images (train/test splits).</li> <li>Applies preprocessing (resizing, normalization) and optional augmentations (rotation, blur, color jitter).</li> <li>Loads pixel-level ground-truth masks for defect evaluation.</li> <li>(CLI mode) Generates and saves a synthetic \"drifted\" dataset to disk for robustness testing.</li> </ol>"},{"location":"pipeline/data/#arguments","title":"Arguments","text":"<p>--data_root: Path to the root directory of the dataset.</p> <p>--class_name: The object category to analyze (e.g., carpet).</p> <p>--img_size: Target image size for resizing (default: 224).</p> <p>--batch_size: Batch size for data loading (default: 8).</p> <p>--augment: Flag to enable data augmentation.</p> <p>--aug_types: List of augmentations to apply. Choices: rotation, color, blur.</p> <p>--aug_multiplier: Number of augmentations per image (only used in save mode).</p> <p>--rot_degrees: Maximum degrees for rotation augmentation.</p> <p>--blur_kernel: Kernel size for Gaussian blur.</p> <p>--save_aug_path: Path to save the augmented images. If provided, the script runs in \"save mode\" instead of loading mode.</p> <p>--save_aug_dataset_name: Name of the output dataset folder.</p> <p>--split: Which split to process/save (train or test).</p> <p>--include_anomalies: Flag to include defect folders when generating drifted data for the test split.</p>"},{"location":"pipeline/data/#verify-data-loading","title":"Verify Data Loading","text":"<p>Run this to initialize the dataloaders and check sample counts without saving any files. This confirms the dataset structure is correct.</p> <pre><code>uv run python src/anomaly_detection/data.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --img_size 224 \\\n  --batch_size 8\n</code></pre>"},{"location":"pipeline/data/#folder-structure","title":"Folder structure","text":"<p>Dataset folder structure:</p> <pre><code>data/\n\u2514\u2500\u2500 carpet/\n    \u251c\u2500\u2500 train/\n    \u2502   \u2514\u2500\u2500 good/\n    \u2502       \u251c\u2500\u2500 000.png\n    \u2502       \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 test/\n    \u2502   \u251c\u2500\u2500 good/\n    \u2502   \u251c\u2500\u2500 cut/\n    \u2502   \u251c\u2500\u2500 hole/\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 ground_truth/\n        \u251c\u2500\u2500 cut/\n        \u2502   \u251c\u2500\u2500 000_mask.png\n        \u2502   \u2514\u2500\u2500 ...\n        \u251c\u2500\u2500 hole/\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"pipeline/data/#generate-a-drifted-dataset-data-augmentation","title":"Generate a \"Drifted\" Dataset (data augmentation)","text":"<p>Run this to apply augmentations (color, blur and rotation) to the test set and save the images to disk. This is useful for testing model robustness against domain shifts.</p> <pre><code>uv run ./src/anomaly_detection/data.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --save_aug_path ./data/augmented \\\n  --save_aug_dataset_name carpet_augmented \\\n  --split test \\\n  --augment \\\n  --aug_types rotation color blur \\\n  --aug_multiplier 1 \\\n  --rot_degrees 20 \\\n  --color_brightness 0.2 \\\n  --color_contrast 0.2 \\\n  --blur_kernel 3 \\\n  --include_anomalies\n</code></pre>"},{"location":"pipeline/data_drift/","title":"Data Drift &amp; Data Augmentation Demo","text":"<p>This page describes our data drift experiment and how we evaluate the effect of rebuilding the memory bank after distribution shifts in the data.</p> <p>The goal is to demonstrate how data augmentation\u2013induced drift affects model outputs, and whether retraining on drifted data improves robustness.</p>"},{"location":"pipeline/data_drift/#motivation-why-data-drift","title":"Motivation: Why Data Drift?","text":"<p>In real-world deployments, data distributions often change over time due to:</p> <ul> <li>Lighting changes</li> <li>Color shifts</li> <li>Sensor degradation</li> <li>Environmental variation</li> <li>Domain transfer</li> </ul> <p>This phenomenon is known as data drift.</p> <p>Our anomaly detection model is trained on normal images only using a DINOv3-based memory bank. We want to study what happens when:</p> <ol> <li>The test data is drifted</li> <li>The training data is also drifted</li> <li>The memory bank is rebuilt on drifted data</li> </ol>"},{"location":"pipeline/data_drift/#experimental-idea","title":"Experimental Idea","text":"<p>We compare two scenarios:</p>"},{"location":"pipeline/data_drift/#before-no-adaptation","title":"Before (No Adaptation)","text":"<ul> <li>Test data is artificially drifted</li> <li>Inference uses a memory bank built on original training data</li> <li>Expectation: higher anomaly scores / distribution shift</li> </ul>"},{"location":"pipeline/data_drift/#after-with-adaptation","title":"After (With Adaptation)","text":"<ul> <li>Training data is drifted using the same transformation</li> <li>Memory bank is rebuilt on drifted training data</li> <li>Test inference is repeated</li> <li>Expectation: anomaly score distribution shifts back toward normal</li> </ul>"},{"location":"pipeline/data_drift/#high-level-pipeline","title":"High-Level Pipeline","text":"<pre><code>Generate drifted test data\n        \u2193\nInference on drifted test data\n        \u2193\nHistogram (\"Before\")\n        \u2193\nGenerate drifted training data\n        \u2193\nRebuild memory bank\n        \u2193\nInference on drifted test data again\n        \u2193\nHistogram (\"After\")\n        \u2193\nSide-by-side comparison (demo plot)\n</code></pre> <p>The docker build command must be run once to create the Docker image before the data drift experiment can be executed with docker run.</p>"},{"location":"pipeline/data_drift/#build-a-docker-image","title":"Build  a Docker image","text":"<pre><code>docker build -f data_drift_demo.dockerfile -t mlops-data-drift-demo .\n</code></pre> <p>Run the container:</p> <pre><code>docker run --rm `\n  -v \"$((Get-Location).Path)\\data:/app/data\" `\n  -v \"$((Get-Location).Path)\\models:/app/models\" `\n  -v \"$((Get-Location).Path)\\results:/app/results\" `\n  -e COLOR_CONTRAST=0.6 `\n  -e CLASS_NAME=carpet `\n  -e DATA_ROOT=/app/data `\n  -e OUT_ROOT=/app/results/data_drift `\n  mlops-data-drift-demo\n</code></pre>"},{"location":"pipeline/data_drift/#outputs","title":"Outputs","text":"<p>After completion, the following artifacts are produced:</p> <ul> <li>Drifted datasets (train &amp; test)</li> <li>Heatmaps for both runs</li> <li>Histogram before adaptation</li> <li>Histogram after adaptation</li> <li>Comparison plot (demo.py)</li> </ul>"},{"location":"pipeline/evaluation/","title":"Evaluation pipeline","text":"<p>The evaluation script quantitatively assesses the performance of the anomaly detection model on a labeled dataset. It compares predicted anomaly scores against ground-truth labels to compute evaluation metrics, enabling objective comparison of models, hyperparameters, and configurations in a reproducible way. We used logs on evaluation with key metrics such as image-level and pixel-level ROC AUC. Each run produces a timestamped log file in <code>logs/</code> for traceability.</p> <ul> <li>Metric Calculation: Computes Image-Level ROC AUC and, if masks are available, Pixel-Level ROC AUC.</li> <li>Visualization: Generates histograms of anomaly scores (Good vs. Defective) and Pixel ROC curves.</li> <li>Dual Modes: Can run full inference using a model and memory bank, or quickly re-evaluate using pre-computed scores from a JSONL log.</li> </ul>"},{"location":"pipeline/evaluation/#arguments","title":"Arguments","text":"<p>--data_root: Path to the dataset root folder.</p> <p>--class_name: The specific object category to evaluate.</p> <p>--weights_path: Path to the pretrained DINOv3 model weights.</p> <p>--memory_bank_path: Path to the computed memory bank (.pt file).</p> <p>--output_dir: Directory to save results (plots and ROC curves); defaults to ./results.</p> <p>--k: Number of nearest neighbors to use for anomaly scoring (default: 10).</p> <p>--scores_jsonl: (Optional) Path to a JSONL file with pre-computed scores. If provided, skips inference and only generates histograms/metrics.</p> <p>--hist_only: (Optional) Flag to skip pixel-level evaluation for faster execution.</p>"},{"location":"pipeline/evaluation/#full-evaluation","title":"Full evaluation","text":"<p>This command runs the full pipeline: loading the model, computing anomaly maps for the test set, and calculating both image and pixel-level AUC. <pre><code>uv run src/anomaly_detection/evaluate.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain.pth \\\n  --memory_bank_path ./models/checkpoints/memory_bank_carpet.pt \\\n  --output_dir ./results \\\n  --img_size 224 \\\n  --k 10\n</code></pre></p>"},{"location":"pipeline/evaluation/#fast-re-evaluation-from-logs","title":"Fast Re-Evaluation (From Logs)","text":"<p>If you have already run inference and saved scores, use this to quickly regenerate histograms or check Image AUC without reloading the heavy model. <pre><code>uv run src/anomaly_detection/evaluate.py \\\n  --class_name carpet \\\n  --scores_jsonl ./logs/inference_scores.jsonl \\\n  --output_dir ./results_reanalysis\n</code></pre></p>"},{"location":"pipeline/inference/","title":"Inference pipeline","text":"<p>This script runs the anomaly detection pipeline on new images using a pre-trained DINOv3 model and a pre-computed memory bank. It generates visual outputs (heatmaps and overlays) and quantitative data (anomaly scores) without retraining.</p> <ul> <li>Core Function: Loads a frozen DINOv3 model and a memory bank to compute anomaly scores for each image in the dataset.</li> <li>Visualization: Produces pixel-level anomaly heatmaps and overlays them onto the original images to localize defects.</li> <li>Scoring: Calculates a scalar anomaly score for each image (max value of the anomaly map) and can save these scores to a JSONL file for later analysis.</li> </ul>"},{"location":"pipeline/inference/#what-it-does","title":"What it does:","text":"<ol> <li>Loads a test image and the pre-computed memory bank (.pt file).</li> <li>Loads the frozen DINOv3 feature extractor.</li> <li>Computes anomaly maps by comparing image patches to the memory bank (k-NN).</li> <li>Generates and saves visual outputs (heatmaps and overlays).</li> <li>Calculates per-image anomaly scores and logs them to a JSONL file.</li> </ol>"},{"location":"pipeline/inference/#arguments","title":"Arguments","text":"<p>--data_root: Path to the root directory of the dataset.</p> <p>--class_name: The object category to analyze.</p> <p>--weights_path: Path to the pretrained DINOv3 model weights.</p> <p>--memory_bank_path: Path to the saved memory bank (.pt file).</p> <p>--output_dir: Directory where results will be saved; defaults to ./results/figures.</p> <p>--output_name: (Optional) Custom name for the output folder; defaults to the class name.</p> <p>--split: The dataset split to run inference on (train or test); defaults to test.</p> <p>--save_heatmaps: Flag to enable saving of anomaly heatmaps (enabled by default).</p> <p>--save_overlays: Flag to enable saving of heatmaps overlaid on original images (enabled by default).</p> <p>--heatmaps_only: Convenience flag to disable overlays and save only the heatmaps.</p> <p>--scores_jsonl: (Optional) Path to save per-image anomaly scores and labels in JSONL format.</p> <p>--img_size: Target image size for resizing (default: 224).</p> <p>--batch_size: Batch size for data loading (default: 8).</p> <p>--k: Number of nearest neighbors to use for anomaly scoring (default: 10).</p>"},{"location":"pipeline/inference/#standard-inference-visuals-scores","title":"Standard Inference (Visuals + Scores)","text":"<p>This is the most common use case: generating heatmaps, overlays, and a score log for the test set.</p> <pre><code>uv run src/anomaly_detection/inference.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain.pth \\\n  --memory_bank_path ./models/memory_bank.pt \\\n  --output_dir ./results/figures \\\n  --scores_jsonl ./logs/inference_scores.jsonl \\\n  --save_heatmaps \\\n  --save_overlays \\\n  --k 10\n</code></pre>"},{"location":"pipeline/inference/#heatmaps-only-faster-visualization","title":"Heatmaps Only (Faster Visualization)","text":"<p>Use this when you only need to inspect the raw anomaly signal without the context of the original image (saves disk space and time). <pre><code>uv run src/anomaly_detection/inference.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain.pth \\\n  --memory_bank_path ./models/memory_bank.pt \\\n  --heatmaps_only\n</code></pre></p>"},{"location":"pipeline/train/","title":"Training pipeline (memory bank)","text":"<p>In this Zero-Shot framework, \"training\" does not involve gradient updates. Instead, this script builds a Memory Bank of features from normal data.</p> <ul> <li>Feature Extraction: Runs the frozen DINOv3 model on the training set (only \"good\" images) to extract high-level feature representations.</li> <li>Memory Bank Construction: Aggregates these feature vectors into a single tensor, acting as the \"reference\" for normality.</li> <li>Persistence: Saves the resulting tensor (.pt) to disk, which is required by the inference and evaluation modules.</li> </ul>"},{"location":"pipeline/train/#what-it-does","title":"What it does:","text":"<ol> <li>Loads training images (<code>train/good/</code>)</li> <li>Loads DINOv3 weights</li> <li>Extracts patch features</li> <li>Builds a memory bank tensor</li> <li>Saves it as a <code>.pt</code> file</li> </ol>"},{"location":"pipeline/train/#arguments","title":"Arguments","text":"<p>--data_root: Path to the dataset root directory.</p> <p>--class_name: The specific object category to model.</p> <p>--weights_path: Path to the pretrained DINOv3 model weights.</p> <p>--save_path: File path where the memory bank .pt file will be saved. Defaults to ./models/memory_bank.pt.</p> <p>--img_size: Target image size for resizing (default: 224). Must match the size used during inference.</p> <p>--batch_size: Number of images processed at once (default: 8).</p> <p>--augment: (Optional) Enable data augmentation during memory bank creation. Typically False for standard banks, but can be used to build robust banks.</p> <p>--aug_types: (Optional) List of augmentations to apply if --augment is set (e.g., rotation, color, blur).</p>"},{"location":"pipeline/train/#standard-memory-bank-construction","title":"Standard Memory Bank Construction","text":"<p>This is the default usage: it processes the \"good\" training images and creates a standard reference bank.</p> <pre><code>uv run src/anomaly_detection/train.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain.pth \\\n  --save_path ./models/checkpoints/memory_bank_carpet.pt \\\n  --img_size 224 \\\n  --batch_size 16\n</code></pre>"},{"location":"pipeline/train/#robust-memory-bank-with-augmentation","title":"Robust Memory Bank (with Augmentation)","text":"<p>If you expect the test environment to have specific variations (e.g., slight rotations or lighting changes) that should be considered \"normal,\" you can inject them into the memory bank directly.</p> <pre><code>uv run src/anomaly_detection/train.py \\\n  --data_root ./data/ \\\n  --class_name carpet \\\n  --weights_path ./models/dinov3_vitb16_pretrain.pth \\\n  --save_path ./models/checkpoints/memory_bank_carpet_robust.pt \\\n  --augment \\\n  --aug_types rotation color \\\n  --rot_degrees 5 \\\n  --color_brightness 0.1\n</code></pre>"}]}